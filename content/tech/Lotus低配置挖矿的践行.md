+++
title = "Lotus低配置挖矿的践行"
date = "2020-01-21T14:11:22+08:00"
slug = "create-saved-miner-filecoin-cluster"
description = "Filecoin挖矿集群"
tags = ["ipfs", "decentralized", "Filecoin", "lotus", "矿工"]
gitinfo = true
displayCopyright = false
Categories =  ["ipfs", "decentralized", "Filecoin", "lotus", "矿工"]
+++



# 本次测试的重点

关注的重点一直在于对lotus存储本身的测试和研究，从而了解lotus的稳定性和lotus不同存储过程状态对硬件资源的消耗情况， 最终得到一个数字上最合理的硬件资源模型，挖掘出硬件配置和存储算力产出之间的关系。这需要清晰的知道lotus存储过程的细节， 以及实践得到每个状态转换过程中主要耗费的硬件资源和时间，到最后得出怎样的硬件配比可以被充分利用，而不导致过多的资源闲置。

# 关于lotus的稳定性测试
总体评测如下：

整体测试流程还是比较流畅的，硬件和GPU配置正确的情况下，官方描述的挖矿流程都可以顺利的跑通。
测试期间没有无故的lotus和lotus-storage-miner进程退出，不像最早的Filecoin进程跑了一段时间会无故的退出(大多数情况下是硬件内存被耗尽， 向操作系统申请内存失败导致)。
lotus和lotus-storage-miner在工作期间可以使用ctrl+c安全退出，这个安全的定义是：杀掉进程后，再次启动，存储挖矿流程可以继续进行， 但是会出现部分出于某个状态的sector无法恢复，这个对于正式的文件存储是需要被解决的问题。
sector不同状态转换，会出现一定概率的出错，这个对于正式的文件存储，也是需要解决的问题。
区块链整个流程都比较流畅，区块同步和出块在测试期间没有出现问题。
lotus存储主要的资源耗费

## 测试过程监测到的主要资源耗费如下：

名称	说明
IO	主要是磁盘IO，这个是核心，是最难调和的资源耗费，也是整个集群的每天可以产出的存储算力的第一个瓶颈。
CPU	主要的计算资源，全部存储过程都要用到。
RAM	内存资源，全部存储过程都会用到。
GPU	这个主要用在时空证明，也可以用于封包(复制证明)加速
我们的集群架构
我们采取是一个主节点+30个密封节点的集群形式，密封的是1G的sector。具体的硬件配置如下：

### 主节点 x 1

CPU	8核16线程
内存	32G
硬盘	一个2T的PCIE接口的SSD硬盘 + 2个8T的企业硬盘
网络	千兆网卡 + 内网
GPU	NVIDIA 2080 SUPER (8G显存)

### woker节点 x 30

CPU	20台(4核8线程) + 10台(2核4线程)
内存	16G
硬盘	1个8T的企业硬盘
网络	千兆网卡 + 内网
GPU	无

## lotus的sector转换过程和主要的资源耗费分析
lotus和lotus-storage-miner启动后，从最开始的数据到最后的网络的存储算力，主要的几个过程以及每个过程的耗费的主要的资源如下：

![工作流程](/images/image-b3ad4f9ffccaf33de80e9f1d8630883e.png)


备注：因为测试研究过程需要经常更换硬件和内核配置，此过程的截图数据来自我们本地机器的研究过程，非线上的工作集群， 配置：(8核16线程，1个128G的SSD系统盘，1个512的PCIE硬盘，24G内存，无GPU)。

## 第一阶段：把文件数据(这里是指lotus pledge-sector的数据)分片进行addPieces前的工作。

资源耗费截图：

![资源图](/images/image-6b43ce738dd39e0aba886ec26c141149.png)

结果分析：这个过程主要耗费的是CPU的计算资源，磁盘IO和内存也会稍微偏高，相对CPU资源的增量来说，不算高。

## 第二阶段：将pieces添加到sector得到precommit的扇区。


![工作过程1](/images/image-fb00682205766b0b1a75b93138994326.png)
![工作过程2](/images/image-9a0db3e65430f0855caddb4195bbd839.png)


结果分析：这个截图是单台的测试结果，发现CPU被占满了，内存也飚到了18G，但是还有不少空余，IO也是少量的，只有密封完了后， 回写数据的一段时间IO会偏高，对于单台这个过程主要的瓶颈在CPU，集群模式还得考虑带宽。

重点分析：sector的密封可以分布式进行，例如我们的正式测试环境就使用了30个小节点一起参与并发的密封，只要带宽足够， 整个密封的速度可以线性的增长，同时可以解放一些主节点的计算压力。但是密封节点的数量并不是越多越好， 瓶颈主要在带宽和主节点产出的pre-commit的sector数量的联动，例如：主节点产出的pre-commit数量不够，增加密封节点也是闲置的。

我们线上的密封节点截图如下：


密封节点在拉取和回写数据

![拉取图](/images/image-0da879d4ccb6282d76dddd418ea03df4.png)

Remote 节点数量

![回写图](/images/image-211326ed18ac4c15537856a202ccbd98.png)


remote有30个，同时30个在线并且处于忙碌状态。

集群情况下的带宽使用情况

![带宽使用图](/images/image-b17d2bf426a0e092e57428138f3b81a5.png)

千兆的网卡基本已经处于满负荷的状态了。

## 第四阶段：将进行了密封的扇区进行时空证明。

主要资源耗费截图：

![主要资源耗费图](/images/image-8b759fa07ef13f2adfa5edd9a3a8ecb8.jpeg)

结果分析：这个过程主要耗费的是GPU（CPU效率太低），当然CPU和内存占用也会一定量的增大，这个地方没有截取到整体的监测截图。

配置和存储算力的关系总结
这个为了提高输出量得使用master主节点+n个woker节点的集群方案，主要的五个资源因素是：磁盘IO，CPU，内存，带宽，GPU； 每个因素维度都可以有很多的落地性的优化方案，但是有一点可以确定的是，这几个配置资源的关系一定要合理，不然会出现一些资源被占满， 另一些资源闲置的情况，没法充分利用资源，尤其是主节点的配置，有问题欢迎联系交流探讨。

下面是我在测试过程中的部分录屏的视频，不是很完整，但是大致包含了整个分析过程：

https://v.youku.com/v_show/id_XNDQ3MDkzNTIzNg==.html